[
["index.html", "Computational Tools for Reproducible Science Overview 0.1 Software Requirements and Installation Instructions", " Computational Tools for Reproducible Science Simona Picardi 2021-01-06 Overview This digital book contains the material for the Graduate Special Topics course WILD 6900: Computational Tools for Reproducible Science. The aim of the course is to provide students with practical skills to manage and process their data throughout their life cycle, from the moment they are entered into a computer to the moment they are used in a publication, document, presentation, etc. The content is organized in the following Chapters: Chapter 1, Project Organization Chapter 2, Version Control with Git Chapter 3, Collaborative Science with GitHub Chapter 4, Best Practices in the Use of Spreadsheets Chapter 5, Relational Databases Chapter 6, Basics of SQL Language Chapter 7, Linking Databases and R with RSQLite Chapter 8, Dynamic Documents with RMarkdown Chapter 9, Automatically Generated Websites with GitHub Pages Chapter 10, Introduction to R Chapter 11, Troubleshooting in R Chapter 12, Working Environments in R Chapter 13, Data Wrangling with tidyverse Chapter 14, Data Visualization with ggplot2 Chapter 15, Introduction to Geospatial Data in R Chapter 16, Problem Decomposition 0.1 Software Requirements and Installation Instructions Required software is listed below along with installation instructions for different operating systems. 0.1.1 Git Git is a distributed version control system. It is free and open source. To install Git, follow instructions for your operating system below. Also, make sure you create a GitHub account on https://github.com/. 0.1.1.1 Windows Download from the Git website: go to https://git-scm.com/download/win and the download will start automatically. 0.1.1.2 Mac OS On Mavericks (10.9) or above, when you try to run a Git command from the Terminal for the first time, the installation will start automatically if you don’t already have Git installed. Type the following in the terminal: $ git --version And follow the instructions on the installation wizard. 0.1.1.3 Linux In the command line: $ sudo apt install git-all 0.1.2 Spreadsheet Editor Most people will already have Excel installed on their computer. However, any spreadsheet editor will work for the purpose of this course. If you don’t have access to an Office License, LibreOffice or OpenOffice are free, perfectly viable alternatives to Excel. Download the installer for your operating system: LibreOffice: https://www.libreoffice.org/download/download/ OpenOffice: https://www.openoffice.org/download/ 0.1.3 SQLite SQLite is a lightweight relational database management system. To install it, follow these steps: Go to https://www.sqlite.org/download.html and find your operating system in the list. You are looking for a category called “Precompiled Binaries”. For example, if you are on Windows, look for “Precompiled Binaries for Windows”. From this list, chose the file whose name starts with “sqlite-tools”. The description will read something like, “A bundle of command-line tools for managing SQLite database files, including the command-line shell program, the sqldiff.exe program, and the sqlite3_analyzer.exe program” In your file explorer, create a new folder called “sqlite” (e.g., on Windows, C:) Extract the .zip file you downloaded into this new folder. Download SQLiteStudio (this is a GUI, or Graphical User Interface, that we are going to use to run our SQL commands) here: https://github.com/pawelsalawa/sqlitestudio/releases. Download the file whose name starts with “Install” and choose the .exe extension if you’re working on Windows, .dmg if you’re on Mac OS, and the one without extension if you’re on Linux. If these instructions weren’t clear, you can find more details (with screenshots) at this link: https://www.sqlitetutorial.net/download-install-sqlite/ 0.1.4 R R is a free software environment for statistical computing and graphics. Note that installing or updating R is a separate, independent process from installing or updating RStudio! If you already have R installed, make sure you have the latest available version. Follow installation or update instructions for your operating system below. 0.1.4.1 Windows Download the latest version of R at https://cran.r-project.org/bin/windows/base/ 0.1.4.2 Mac OS Download the latest version of R at https://cran.r-project.org/bin/macosx/ 0.1.4.3 Linux These instructions are for Ubuntu 18.04. If you are running a different version of Debian/Ubuntu, there are some small adjustments to make (see below). In the command line, add the GPG Key: $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 Add the R repository (here is where you have to replace the appropriate release name if you’re working with a different version of Ubuntu; you can find the complete list here: https://cloud.r-project.org/bin/linux/ubuntu/): sudo add-apt-repository &#39;deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/&#39; Update package lists: $ sudo apt update Install R: $ sudo apt install r-base 0.1.5 RStudio RStudio is a free Integrated Development Environment (IDE) for R. Note that installing or updating RStudio is a separate, independent process from installing or updating R! If you already have RStudio installed, make sure you have the latest available version. Otherwise, go ahead and download it from here: https://rstudio.com/products/rstudio/download/#download (choose the appropriate version for your operating system.) 0.1.6 Required R Packages Throughout the course, we will be using the following R packages: RSQLite, rmarkdown, bookdown, renv, tidyverse, lubridate, raster, and sf. All these packages are on CRAN and can be installed (along with their dependencies) by running the following code in R: install.packages(c(&quot;RSQLite&quot;, &quot;rmarkdown&quot;, &quot;bookdown&quot;, &quot;renv&quot;, &quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;raster&quot;, &quot;sf&quot;), dependencies = TRUE) "],
["project-organization.html", "Chapter 1 Project Organization 1.1 Directory structure 1.2 Golden rules 1.3 Be flexible 1.4 Documentation 1.5 Naming files 1.6 RStudio Projects 1.7 Relative and absolute paths", " Chapter 1 Project Organization 1.1 Directory structure When starting a new project, it is worthwhile to spend some time thinking about how to best organize its content. This means asking questions such as: What are my project inputs? What types of outputs do I expect to come out of this project? How does this project relate to other projects I am working on? Answering these questions can help us decide on a directory structure to house our project files. For example, say that we are working on a research project that entails data cleaning, analysis, and writing of a manuscript. One way to go about it is to have a root project directory that contains subfolders for data, analysis code, results, and the manuscript file/s. We can call this a “project-based directory layout.” Alternatively, if we have multiple projects underway, we can split our files by activity rather than by projects and have a “data” folder, an “analyses” folder, a “manuscripts” folder, etc., all with a subfolder for each project. We can call this an “activity-based directory layout.” Figure 1.1: Project-based directory organization Figure 1.2: Activity-based directory organization How you define a project is largely up to you. It could be all the work you do on a given dataset, or all the work you do for a thesis or dissertation, or each manuscript could be its own project even if it uses the same dataset as a different project. With so many options, how do we choose? Ultimately, the way you structure your project directories will have a big impact on the efficiency and reproducibility of your work, so there are some criteria to keep in mind. 1.1.1 Overlap The first thing to consider is the overlap in data and code files between a project and other past/current projects. If two projects don’t share any data, it’s probably best to manage them separately. If two projects share a large amount of data, it might make more sense to manage them together as one. If you are writing functions to use across several different projects, they should probably be in their own project. 1.1.2 Minimizing duplication The reason why it’s important to account for overlap is because you want to minimize duplication as much as you can. Duplication of both data and code files is dangerous because it’s easy to lose track of modifications you have made to a file saved in one place but not in the other place. As you keep making changes, the two files will have the same name but different content, which generates confusion. Duplication is also inefficient because it occupies precious space on your computer. 1.1.3 Self-containedness A fundamental quality of reproducible projects is that they are self-contained, meaning that ideally you could zip up the entire project directory and send it to a friend and they should be able to run your code and reproduce your results without changing anything. This means everything the project needs to work, from A to Z – data, functions, code – is contained in its root directory. 1.1.4 Tradeoffs The self-containedness criterion is sometimes in contradiction with minimizing duplication: if two projects share one or more data files, you can either violate the duplication criterion by making sure each project contains all the necessary data for the sake of self-containedness; or you can choose to sacrifice self-containedness to avoid file duplication and save space on your computer. The answer will depend on whether you anticipate the project to be widely shared or mostly for your personal use, how large the shared data files are, etc. The bottom line is that there is no one-size-fits-all solution for project organization. Most importantly, the structure you choose needs to be functional for your needs. Putting thought into it is a great place to start. 1.2 Golden rules Whichever directory structure you choose for a project, there are some universal rules to keep in mind for how to organize files within it. First and foremost, raw data should never be changed. Save it into a “data” folder and treat it as immutable. You can even set it as read-only to make sure there is no room for accidents. The processed, clean version of your data will go into a dedicated “processed_data” folder. Anything that can be generated from code goes into its own folder. This includes basically everything but the raw data and the code itself. You can have an “output” folder, or separate folders for output files and figures (e.g., “output” and “figures”.) If there are text documents, put them in their own folder (e.g., “docs”) Code also has its own folder. If you write a lot of functions, it can be helpful to have a “funs” folder to store those and a “src” (for ‘source’) folder to save processing/analysis code. If processing/analysis scripts are meant to be used in a certain order, you can number them (more on this in a minute.) Sometimes the pipeline is not linear but branched, so numbering may not always make sense. Function scripts should not be numbered. Modularize your code: instead of having a giant script to run your entire analysis from data cleaning to final figures, break up your workflow into several short, single-purpose scripts with well-defined inputs and outputs. 1.3 Be flexible It can be challenging to anticipate the structure of a project that is just about to start (especially the first time you start thinking through optimal directory structures.) It helps to be flexible and allow some room for adjustments. For example, you can start with a basic directory structure where you have a “data” folder, a “code” folder, and an “output” folder, and then you may decide to split the “code” folder into “src” and “funs”, or to further split the “output” folder into “output” and “figures”, etc. Sometimes these changes can break your code and become frustrating, but these problems are easy to fix. Similarly, it can be challenging to have the long-term vision to know where it’s best to break a script and start a new one. It’s easy to tunnel vision into an analysis and keep adding lines and lines of code without thinking about break points that make sense. One good way to deal with this is to reserve some time at the end of your coding session to look at your script and notice if there are any intermediate products that you can save and use as input of the next step of the workflow. Then you can make the necessary adjustments in terms of code compartmentalization, paths to input and output files, etc. In the words of Wilson et al. (2017, see references below), “consistency and predictability are more important than hairsplitting” when organizing the directory structure for your projects. Besides a few universal rules, designing the optimal structure often requires consideration of project specifics. Ultimately, the goal is to improve efficiency – by allowing you to find your files easily, only run the minimal amount of code you need for a task, making it easy for collaborators and future you to wrap your head around the project content – and reproducibility. 1.4 Documentation One very important aspect of reproducibility is good documentation. Each of your projects should always be accompanied by a README file. The README should contain all the information an outsider would need to understand what the project is all about, what are the inputs and outputs, where to find files within the project directory, etc. The README is simply a text file, there’s nothing special to it – you can just create it in your Notepad or other text editor. Writing down everything about a project in a README can be tedious, but it pays off ten-fold. It’s good to get in the habit of starting a project by creating a README file right after the directory structure is created. Record who the author/s is/are, the date the project was started, and a description of what the project is for. Then, any time a new file is added, specify what it is and where you got it from. For example, “File X.csv was sent by Mary White in an email on 2/1/2019 to my.address @ gmail.com”, etc. Be meticulous: what seems obvious today can become a puzzle to solve in a few months. 1.5 Naming files There is a science to choosing good file names, too. Here is a list of file names that are not good, for a variety of different reasons: data.csv data_cleaned_03-22-2012.csv analysis code.R Green Frogs Manuscript_Final_edits.docx final.docx Why are those names bad, and what makes a good file name? Good file names are computer-readable, human-readable, and work well with default ordering. Let’s break these three criteria down one by one. 1.5.1 Computer-readable file names What makes a file name computer-readable? First, computer readable files contain no spaces, no punctuation, and no special characters. They are case-consistent, which means that you always stick to the same case pattern, whether that be full lowercase, camel case (ThisIsWhatIMeanByCamelCase), or whatever else. Finally, good file names make deliberate use of text delimiters. Wise use of delimiters makes it easy to look for patterns when you are searching for a specific file. Usually, it’s recommended that you use an underscore (_) to delimit metadata units and a dash (-) to delimit words within a metadata unit. For example, here is a good, computer-readable file name: 2018-04-26_reproducible-science_slides_lesson-01.pptx 1.5.2 Human-readable file names The example file name above is not only computer-readable, it’s also human-readable. This means that a human can read the file name and have a pretty good idea of what’s in that file. Good file names are informative! You shouldn’t be afraid to use long names if that’s what it takes to make them descriptive. 1.5.3 File names that work well with default ordering If you sort your files by name in a folder, you want them to be ordered in a way that makes sense. Whether you sort your files by date or by a sequential number, the number always goes first. For dates, use the YMD format, or your files created in April of 1984 and 2020 will be closer than the ones created in March and April 2020. If you are using sequential numbering, add a sensible amount of zeros in front based on how many files of that category you expect to have in the future. If you expect to have more than 10 but not more than 99 files, you can add a single leading zero (e.g., “data_analysis_01.R” instead of “data_analysis_1.R”), whereas if you expect to have between 100 and 999 you can add two (e.g., “Photo_001.jpeg” instead of “Photo_1.jpeg” or “Photo_01.jpeg”.) 1.6 RStudio Projects RStudio Projects are a great tool to help you stay organized. The concept behind RStudio Projects is that each Project is a self-contained unit where inputs, outputs, and code are all in one place. Sounds familiar? RStudio Projects work seamlessly with the directory organization framework we have been talking about. It’s a good idea to make sure each of the projects (with a lowercase p) you work on has its own associated RStudio Project (with a capital P). Figure 1.3: How to create a new RStudio Project You may be familiar with the concept of a working directory in R. The working directory is the place where, unless otherwise specified, all of your outputs are saved. All relative paths are also interpreted relative to the working directory (more on relative paths in a minute.) The cool thing about an RStudio Project is that it automatically makes sure that your project directory is set as the working directory. All the clunkiness of having to set, change, or double check which directory you’re working in is forgotten: all you do is open up your Project and then the paths you use to load or save data are already, by default, relative to your project directory. 1.7 Relative and absolute paths Paths define the location of files within the file system. There are two ways to point to a file from within a command prompt such as R. The first is to use what is called an absolute path: this describes the full sequence of folders a file is contained in, starting from the root directory of a computer. When you right click on any file on your computer, you can look up its absolute path in the Properties. This is an example of an absolute path: C:/Users/MJS/Documents/PhD/Planning/schedule_2021.csv Relative paths describe the position of a file with respect to a reference directory. In R, that reference directory is your working directory. When you type a relative path to open a file in R, R appends that path to the path of the working directory to get the full absolute path, which it then uses to open the file. For example, if our working directory was “Documents”, this would be the relative path to reach the same file as above: PhD/Planning/schedule_2021.csv If our working directory was “Planning”, this would be the relative path to the same file: schedule_2021.csv To work across subfolders of our working directory, we can just use relative paths to navigate and locate files. But relative paths also work to navigate across folders that are outside of the working directory, if need be. Using “../” in front of a folder name navigates to the parent of that folder (“parent” means the folder where that is contained.) Let’s consider the following directory structure: Figure 1.4: Example directory structure We can navigate to a file into “PhD/Research” from “Planning” like so: ../Research/GreenFrogs/data-cleaning.RProj The “../” means “the parent directory of the working directory”, which is “PhD”. We can also stack these to navigate further up the directory tree: ../../../../DAJ The path above navigates all the way up to “Users” and into a different user’s directory. 1.7.1 Path separators Something to be mindful of is that the separator between folder names in a path is different on different operating systems. On Windows, it is a backslash (“\"), while on Mac/Linux, it is a slash (”/“). However, R uses the Mac/Linux convention, so always use”/\" when typing paths in R. 1.7.2 References Wilson G, Bryan J, Cranston K, Kitzes J, Nederbragt L, Teal TK (2017) Good enough practices in scientific computing. PLoS Comput Biol 13(6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510 https://speakerdeck.com/jennybc/how-to-name-files https://r4ds.had.co.nz/workflow-projects.html "],
["version-control-git.html", "Chapter 2 Version Control with Git 2.1 Command line basics 2.2 Configuring Git 2.3 Getting help 2.4 Creating a repository 2.5 Tracking files 2.6 Ignoring files 2.7 The 3 states 2.8 Un-staging files 2.9 Recovering a previous version of a file 2.10 Removing Git 2.11 Git everyday 2.12 References", " Chapter 2 Version Control with Git Git is a version control system. It tracks all the changes you make to your files and allows you to go back to previous versions as far back in time as you need. Git is useful for a variety of reasons. Most of us are familiar with folders that look like this: Messy folder Saving multiple versions of files like this is problematic because, first, it uses space inefficiently (even if files are small, multiple versions add up and end up wasting a lot of room on our computers); second, and perhaps most importantly, it creates confusion and makes our workflow prone to errors. Instead of manually saving edits to a file in a separate copy “just in case”, and instead of having to keep track of which is the most recent version, we can put our files under version control and let Git keep track of the changes we make. Git will record a history of changes to each file without duplicating it, and we’ll be able to revert to any previous version in the history of that file. Version control allows us to be clean and tidy in our file organization, optimizing use of space without ever fearing to lose any of our work. Once a file is tracked by Git, it is virtually impossible to do anything irreversible to it. I haven’t mentioned anything yet about the benefits of version control for collaborative work. This is because at its core, Git works locally on your computer. We often tend to think of version control as the way to handle multiple users working on the same file simultaneously, but Git is first and foremost a tool for your own file organization. Git is installed and runs on your machine, not on a network. It is the integration with an external server, be it private or public (GitHub and GitLab, for example, are public server providers), that makes Git a tool for multiple-user applications. This chapter focuses on the local functionalities of Git, while we’ll dive into its benefits for collaborative projects in the next Chapter. 2.1 Command line basics You can use Git from the command line in the computer’s terminal. The command line can be intimidating if you haven’t used it before, but using Git only requires basic familiarity with it. Commands are slightly different between operating systems, so I’ll assume Windows is the most commonly used and specify the Mac/Linux alternative when applicable. When you open up the terminal (or command prompt) on your computer, you’ll see a symbol, on many computers it is a “$” or “&gt;”, followed by a blinking cursor. That symbol is called the prompt, and it means the terminal is waiting for your input. If you copy-paste any code from this chapter into your terminal, make sure you only copy the part after the prompt. Also, if Ctrl+V does not work in the terminal, you can right-click to paste. When you open the terminal, you should automatically be located in the root directory of your file system (or your home directory, if you have a computer with multiple users). The name of the folder you’re in appears right before the prompt. For example, this is what my prompt looks like: C:\\Users\\Simona&gt; If you are not in your root directory when you first open up the terminal, you can use the following command to navigate to it: &gt; cd \\ This command stands for “change directory”. The “\" symbol indicates the root directory. On Mac/Linux, you can navigate to the root directory by simply typing”cd\" or: &gt; cd ~ If you want to go to a specific directory other than the root, you can type its path (relative to the folder you’re currently in) after “cd”. To go from Usersto Users: &gt; cd Documents If you want to go back to the parent of the current directory (the folder that contains it), you can use: &gt; cd .. To go back to Users from Documents: &gt; cd ..\\.. To go back to Documents from Users: &gt; cd Simona\\Documents 2.2 Configuring Git Once Git is installed on our computer, we need to do a few one-time steps to configure it. Let’s set up our name and our email address by typing the following in the terminal: &gt; git config --global user.name &quot;First Last&quot; &gt; git config --global user.email &quot;first.last@example.com&quot; This will be important when we start using GitHub. If you want to change these options in the future you always can, and if you want to use different options than the global ones within a specific project you can run the commands above removing “–global” while in the project directory (not the root!). There are more configuration options you can personalize, but we won’t get into that in this chapter. If you want to check what configuration options you have active, you can use: &gt; git config --list 2.3 Getting help If you need help while using Git, you can use any of the following commands to get the manual page for any of the Git verbs: &gt; git help &lt;verb&gt; &gt; git &lt;verb&gt; --help &gt; man git-&lt;verb&gt; 2.4 Creating a repository Once Git is set up and configured, we are ready to create our first repository. A repository is a directory that is under version control. Let’s use the example directory structure from Figure 1.4. First, we navigate to the folder where we want to initialize our repository. In our example’s case, that is: &gt; cd Documents\\PhD\\Research\\GreenFrogs Now we will enable Git to start tracking everything inside this folder: &gt; git init This command initializes Git. It may appear like nothing changed, but if you allow the option to show hidden files in your file explorer you will notice there is a new subfolder called .git. That folder is where Git will store all of its version control information. You don’t have to worry about the content of that folder and you can keep using your files normally like you always would. 2.5 Tracking files Creating a repository enables Git to start tracking files within it, but that does not happen automatically. We have to tell Git which files we want to track. We can check what Git is tracking so far by using: &gt; git status At the bottom, we’ll see a list of untracked files. We need to switch on tracking on those. To begin tracking a new file, we use the verb ‘add’. For example, to track a file named “green_frog_diet_data_cleaning.R” we would type: &gt; git add green_frog_diet_data_cleaning.R This works well if we want to add a specific file. If we want to start tracking the whole content of the folder, we can do: &gt; git add --all But be careful when using this command: Git is optimized to work with plain text files (for example .txt or R scripts), and it doesn’t really understand binary files (which is how Word files are stored, for example). Also, some files typically do not need to be version controlled, such as images; in fact, because they are large files, version controlling images can end up clogging your workflow. Make sure you are always aware of what exactly you’re adding when you use ‘git add –all’. When in doubt, add files one by one. There are also ways to make this process a little more distraction-proof: in the next step, we’ll see how to make sure we only track the files we want. 2.6 Ignoring files We can set up some rules to exclude from version control files that it would be superfluous or detrimental to track. Writing these down as rules can save us some time and headaches versus having to decide manually every time. We can exclude files from version control within a repository by using .gitignore. This is simply a text file that lists all the rules for files that, by default, we do not want to track. Rules can be based on the file type or patterns in the file name. Generally, there are a few criteria you can consider when deciding what to exclude from version control: File encoding (plain-text vs. binary): Git cannot track changes within binary files, so, even though you can store these files under version control, you won’t be able to use Git to compare different version, so there’s really no point in tracking these; Code-generated: anything that can be reproduced by running code does not need to be version-controlled; Size: files that are too big will slow down the functioning of Git. As a benchmark, you can keep in mind the maximum size limit enforced by GitHub, which is 100 MB – but if you follow the two criteria above, you will rarely end up with this problem because 100 MB’s worth of plain-text files is a whole lot of plain text. You can create a text file called “.gitignore” in our repository by using your default text editor (Notepad for Windows, TextEdit for MacOS, etc). The name must be exactly “.gitignore” for Git to recognize it. The file must have no extension (i.e., .txt) so go ahead and delete that (don’t worry about any warnings). Once .gitignore is created, we can start adding rules. Nothing prevents us from listing files in .gitignore one by one, but this approach is not efficient: stacked_barplot_diet_composition.jpg &lt;&gt; predicted_population_trends.jpg &lt;&gt; individual_movement_tracks.jpg &lt;&gt; capture_locations.jpg &lt;&gt; green_frog_diet_manuscript.docx &lt;&gt; green_frog_movement_manuscript.docx &lt;&gt; green_frog_demography_manuscript.docx Instead, we can use pattern matching to kill many birds with one stone. What all these files have in common is they are all either .jpg’s or .docx’s. We can use the wildcard ’*’ to signify “any character” before the file extension: .jpg &lt;&gt; .docx This will exclude any .png or .docx file from being tracked by Git in this repository. Since the images are conveniently located all together in one folder, we can also just do this: figures/ We should also add the following rules to ignore the user-specific R project files: *.Rhistory /.Rproj.user/ We can add as many rules as we like, then save the .gitignore text file when we’re done. Now, if we didn’t forget to include anything that needed to be ignored, we can safely add all our files in one go: &gt; git add --all 2.7 The 3 states Once files are tracked by Git, they can be in one of three states: modified, staged, or committed. A modified file includes edits that have not been recorded in Git yet; when we stage a modified file, we mark it as ready to be committed; when we commit that staged file, the current version gets stored in the history of that file. Git works with snapshots called “commits”, which are records of what the repository looked like at a certain point in time. By committing a file, we freeze that version of the file forever in its history and we will be able to go back to it. Then, if we edit the file again, we’ll have to tell Git when we’re ready to make a new commit by moving that file to the staging area. Note that the same command ‘git add’ is used both to start tracking a previously untracked file and to add a modified file to the staging area. Now we are ready to do our first commit. We use the following command: &gt; git commit -m &quot;First commit&quot; Each commit should be accompanied by a message, added with the flag -m. The message should describe the changes made to the file/s since the previous commit. It is a good habit to write detailed commit messages, so that when we need to go back and recover a previous version of a file, we can read the history of commits and easily find the version we are looking for. 2.8 Un-staging files It happens to the best of us. We forgot to add a certain rule to .gitignore and when we go ahead and get ready to commit our files we realized we just staged a file we didn’t want to stage. No worries, there is a way to fix that. The following command un-stages files and saves us from having to commit them: &gt; git rm --cached filename You can use pattern matching here as well, but you’ll need to put a backslash in front of the wildcard: &gt; git rm --cached \\*.jpg 2.9 Recovering a previous version of a file Git works like a time machine, allowing us to recover any previous version of a tracked file. It does so by saving snapshots of what the file looked like at each commit. So, each commit represent a point in time that we can access to retrieve the version that existed at that time. We can take a look at the commit history using: &gt; git log Here’s where current me will be grateful to past me for writing descriptive commit messages. If the commit messages do a good job of describing what changes I made to a file, it will be easy to recognize which commit is the one I want to go back to. The string of numbers and letters following the word ‘commit’ in the log is called the hash and it uniquely identifies each commit. We can revert to the version of a file from a specific commit by using the hash as follows: &gt; git checkout 9c5d9a3f52b7d43c1c0a06a94b11df5c3051ca27 -- filename.ext 2.10 Removing Git If for any reason you ever want to remove Git tracking from a repository (going back to a regular folder without version control) you can use the following command: &gt; rm -rf .git 2.11 Git everyday This chapter walked you through commands to configure git, initialize repositories, stage and un-stage files, commit changes, and revert to previous versions of a file. In practice, you may use some of these commands only once (e.g., to configure your email address when you first install Git) or when you start a new repository. In your everyday use of Git, the most important commands you need to remember are those to add and commit your changes. For everything else, feel free to refer back to this manual or other resources on the internet. 2.12 References The Carpentries lesson on Version Control with Git: http://swcarpentry.github.io/git-novice/ Pro Git book by Scott Chacon and Ben Straub: https://git-scm.com/book/en/v2 "],
["github.html", "Chapter 3 Collaborative Science with GitHub 3.1 Adding a remote to a local repository 3.2 Pushing to a remote repository 3.3 Cloning a repository 3.4 Synchronizing changes among collaborators 3.5 Resolving conflicts 3.6 Avoiding conflicts 3.7 Working with branches 3.8 Forking a repository 3.9 Pull requests 3.10 References", " Chapter 3 Collaborative Science with GitHub So far, we have gained a good understanding of how Git works locally on our computer. Now we are going to see how to work with remote repositories and use Git to collaborate with others. A remote repository is a copy of a repository that is stored elsewhere than your local copy, such as a web server or a private network server. In most cases, when collaborating, you’ll have a copy of a repository on your machine (the local repository) and a copy on a server that is accessible to others (the remote repository). The remote repository can be hosted on GitHub. There are two ways to set up your local and remote repositories to interact. One is to set up the local first and link it with a remote later. The second one is to create the remote first and “clone it” on your computer to get a local copy. If you’re starting a brand new repository you can go either way, but the first approach is what you would do if you wanted to link a repository that already exists on your local machine to a GitHub repository. 3.1 Adding a remote to a local repository Starting from an existing Git repository on your computer, you can link this to its GitHub counterpart by adding a remote URL. To set up a URL, go on the GitHub website, click on the ‘+’ sign in the top-right corner and select ‘New Repository’. img You can choose any name for your repository, but I find it intuitive to give it the same name as the folder where my local Git repository is. Since the remote repository will need to be connected to a local one, it needs to be completely empty: if there are conflicts in the content of the two repositories to begin with, Git won’t be able to link them. Therefore, do not add a README, a .gitignore file, or a license. img To connect the local and remote repositories, you can copy the URL of the remote repository and paste it in the following command in the terminal: &gt; git remote add origin https://github.com/picardis/myrepo.git ‘Origin’ is the conventional name given to a remote repository. You could use any other name, but since GitHub uses ‘origin’ as the default name when creating a repository from the website, using this convention will make things easier later. You’d need a very good reason to use a different name. To check that the remote was added correctly you can view the list of remotes associated with your local repository: &gt; git remote -v We’ll talk about branches later in this chapter, and we’ll see that a repository can have multiple branches. At a minimum, each repository includes one main branch, which is created by default when you create your repository. By default, this branch is called master. Starting in 2020, GitHub has started to switch to using main instead of master to remove unnecessary references to slavery. We like this change, because Black Lives Matter. So we’ll go ahead and rename the master branch to main: &gt; git branch -M main 3.2 Pushing to a remote repository Now it’s time to transfer an exact copy of the files in the local repository to the remote one. This action is called ‘pushing’. The first time you push, it’s a good idea to specify which branch Git should use as the default remote for the local main branch in the future. We do this by adding the flag ‘-u’ (for upstream), followed by the name of the remote and the name of the local branch that you want to link up: &gt; git push -u origin main This command is identical to the following: &gt; git push --set-upstream origin main You only have to set the upstream the first time you push a certain branch. After the upstream is set up, you will be able to just push changes to the remote repository by doing: &gt; git push You will be prompted to enter your GitHub password for the push to go through. Once you push, all the files you committed are transferred and your two repositories will mirror each other. To keep the remote repository synchronized with the local, any time you commit changes to tracked files you will also need to push them. That adds a new step to our workflow: add/stage, commit, push. 3.3 Cloning a repository The other way to get a local-remote repository pair set up is to create a GitHub repository first and clone it on your computer. In this case, it doesn’t matter whether the remote repository is empty or not, so you can add a README. Cloning a repository is also useful if you are collaborating on a project with someone on their repository. You and your collaborator/s can all have a copy of the same repository on each of your computers by cloning a shared remote repository, which will then be synchronized with changes made by anyone on the team. The person who created the repository will need to add the others as collaborators so that they’re able to clone it. To clone a repository, open the terminal and navigate to the folder where you want to download it. Then use the following command: $ git clone https://github.com/picardis/myrepo.git The repository will be copied to your computer into the folder you specified. 3.4 Synchronizing changes among collaborators The inverse of pushing is pulling, which transfers files and changes from the remote repository to the local one: $ git pull When you are working with collaborators on a shared GitHub repository, you will periodically need to pull from it to make sure your local copy is synchronized with changes somebody else might have made. You certainly need to pull before you push your own changes, because Git won’t allow you to push if the remote repository has changes that you don’t have on your local copy. If the two repositories are already up-to-date and you try to pull, nothing will happen because there are no changes on the remote repository that aren’t already in the local. If a collaborator makes a change that is in conflict with what we committed to our own local repository, when we try to pull we are going to encounter a merge conflict. Merge conflicts occur when two collaborators work on the same file at the same time. GitHub does not know how to automatically reconcile changes to the same file, and it will throw an error that we’ll need to resolve manually. 3.5 Resolving conflicts Having to deal with merge conflicts is eventually very likely when working on collaborative projects, but they can be handled and resolved without too much pain. If two collaborators edit the same file at the same time without pulling each other’s changes first, when one tries to push their changes to the remote they will get an error message that looks like this: To https://github.com/picardis/myrepo.git ! [rejected] master -&gt; master (fetch first) error: failed to push some refs to &#39;https://github.com/picardis/myrepo.git&#39; hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., &#39;git pull ...&#39;) before pushing again. hint: See the &#39;Note about fast-forwards&#39; in &#39;git push --help&#39; for details. Git is rejecting the push because it found some updates in the remote repository that have not been incorporated in the local copy. To solve this, the person who got the error will need to pull the most recent changes, merge them into their current copy, and then push the resulting file. After pulling, if we open the file where the conflict is, we’ll find that Git did not erase one person’s changes in favor of the other’s; instead, it marked the lines where conflicting changes occur. The beginning and end of the problematic chunk are highlighted, and the two versions are separated by ‘=’ signs. Now it is up to us to reconcile these changes however we consider appropriate. We could keep our own changes, keep the changes made by our collaborator, write something new to replace both, or delete the change entirely. Once the conflict is removed and the file is saved, we can proceed as usual by staging the file, committing it, and pushing it. Now, when any of the other collaborators pulls from the repository again, they will get the merged version of this file. In some cases, collaborators will make simultaneous changes to a file that are mutually exclusive – for example, one person will delete a line of code while another person will edit that same line. However, changes don’t need to be mutually exclusive for a merge conflict to happen: any time two users edit the same line of the same file at the same time (for example, two users may both add a new argument to the same function), Git is going to play it safe and call a merge conflict regardless of whether the changes contradict each other or not. Conflict resolution requires the user to verify whether the changes affect each other and if they can both stay or not. 3.6 Avoiding conflicts Merge conflicts need to be manually resolved by a person, which is time-consuming. More importantly, the person who resolves a merge conflicts is making a judgement call on what is the best way of fixing the situation. Having to introduce subjective judgement calls is perhaps the weakest spot of the entire collaborative version control workflow. Preventing conflicts from happening in the first place is better than having to fix them. There are a few things that can help prevent merge conflicts: Pull often: decreases probability that your working version is not up-to-date; Make small and frequent commits: decreases probability that your recent changes are not in other people’s working version; Organize your code in small modules: decreases probability of two users working on the same file; Working on branches or forks… more on this in the next sections. 3.7 Working with branches By default, a new repository contains a single branch: the main branch. However, we can create new branches within a repository. A branch is like a parallel universe where any changes we make on the files only exist on that branch and do not affect the main branch. Branches allow you to freely experiment with editing files and code without affecting the original project until you’re ready to merge them. The command to create a branch is: &gt; git branch my-branch To start working on the new branch, we switch to it with the following command: &gt; git checkout my-branch Or we could also create the branch and switch to it all in one: &gt; git checkout -b my-branch Where “b” stands for branch. To verify which branch we’re on, we can use ‘git log’ (we’ve used this same command before to look at the commit history): &gt; git log The word ‘HEAD’ in the output of git log is called a pointer; it tells us which branch we’re currently working on. On creation, a new branch shares the commit history of the main branch up until the moment the branch was created. After that, the two commit histories are allowed to diverge. Any changes we commit to a branch will be only effective on the that branch and not affect the others. This takes off all the pressure of making changes to your code that could potentially break your workflow downstream. Once you’ve had a chance to verify that the changes work the way you want them to, you can merge the branches back into one. To merge changes made in a branch into the main branch we use: &gt; git merge my-branch And then we can delete the ‘my-branch’ branch which is no longer needed: $ git branch -d my-branch If there are conflicting changes on the two branches we are trying to merge, Git will halt the process and issue a merge conflict similar to what we have seen before. 3.8 Forking a repository Forking a repository means creating a copy of an existing repository where you can make changes without affecting the original repository. You can technically fork your own repository, but forking is mostly used to get a copy of somebody else’s repository. For example, if you want to contribute to a project that was created by a person you don’t know, or if you want to build off of their code because it does something similar to what you need to do, you can fork their repository and work on it without their copy being affected. The cool thing about forking is that, if the owner makes changes to their repository, you can pull those changes to your fork so that your copy of the repository stays in sync with the original. Also, you can contribute your own changes by submitting them to the original repository through pull requests. This is the main difference between forking and cloning somebody else’s repository: by cloning, you won’t be able to update your copy to include new changes made in the original and you won’t be able to contribute back unless you’re added as a collaborator. Unlike all the other functionalities we have seen so far, which are Git commands, forking is a GitHub functionality. There is no built-in Git function to fork a repository from the command line. To fork a repository, go to the web page of that repository and click “Fork” in the top-right corner. This will create a copy of the repository on your GitHub account. img Now, if you also want a local copy of this repository on your computer, you can clone your fork. For example, if I forked octocat’s Spoon-Knife repository onto my GitHub account, I could then clone it onto my computer like so: &gt; git clone https://github.com/picardis/Spoon-Knife To keep my fork up-to-date with the original repository, I can configure it as follows: $ git remote add upstream https://github.com/octocat/Spoon-Knife.git Now I will be able to pull from the original repository and keep my local copy synchronized. I won’t be able to push because I do not have collaborator privileges on this repository. 3.9 Pull requests Pull requests are the way to ask that the edits you made in a fork (or a branch) get merged into the original repository (or main branch). To ask the owner of a repository to view and potentially integrate the changes you made in your own forked version, you submit a pull request. Similarly, to let your collaborators know about the edits you made and let them review them before they get merged, you submit a pull request. There is a key difference here: while as an external agent you have no other choice but submitting a pull request to merge a fork into a repository, as a collaborator you could simply merge your edits if you wanted to. If you are working on your own repository by yourself, there is no need to go through pull requests to merge a branch. But if you are collaborating with people, it is still good and courteous practice to use pull requests as a heads-up before you merge your changes, so that others on the project can review and approve them. The most straightforward way to submit a pull request is from the GitHub website. To start a pull request, you must have some changes committed to the repository or branch where you want to merge them. Then you can go to the repository page on GitHub and click on the ‘Pull requests’ tab, then click ‘New pull request’. Choose your target branch, enter a title and description, and then click ‘Send pull request’. Once the pull request is open, everyone can write comments on it, see all the commits it includes, and look at the changes it proposes within files. img Once everyone is happy with the proposed changes, you are ready to merge the pull request. If there are no merge conflicts, you will see a ‘Merge pull request’ button. 3.10 References GitHub Community Forum: https://github.community/ "],
["spreadsheets.html", "Chapter 4 Best Practices in the Use of Spreadsheets 4.1 What are spreadsheets good for? 4.2 Human-readable vs. computer-readable data 4.3 Tidy data 4.4 Problematic practices 4.5 Document, document, document! 4.6 References", " Chapter 4 Best Practices in the Use of Spreadsheets So you think you know everything about spreadsheets… think again! While probably all of us have used spreadsheets before, it’s easy to misuse them without even knowing you are doing it. This Chapter gives an overview of best practices and common mistakes to avoid. 4.1 What are spreadsheets good for? Spreadsheets are good tools for data entry. Even though you could technically run some data analyses within your spreadsheet program, that doesn’t mean you should! The main downside to doing data processing and analysis in a spreadsheet is that it entails a lot of pointing and clicking. This makes your analysis difficult to reproduce. If you need to re-run your analysis or re-process some data, you have to do it manually all over again. Also, if you go back to your completed analysis after a while (for example to write the methods of your paper) and you don’t remember what you did, there is no record of it left. This is why this course introduces spreadsheets exclusively as a tool for data entry. 4.2 Human-readable vs. computer-readable data Most of the problematic practices outlined in this Chapter can be summarized into a single concept: using spreadsheets to convey information in a way that is readable for humans but not for computers. While a human brain is capable of extracting information from context, such as spatial layout, colors, footnotes, etc., a computer is very literal and does not understand any of that. To make good use of spreadsheets, we need to put ourselves in the computer’s mind. All the computer understands from a spreadsheet is the information encoded in rows and columns. So, the number one rule to remember when using spreadsheets is to make your data “tidy”. 4.3 Tidy data The fundamental definition of tidy data is quite simple: one variable per column and one observation per row. Variables are the things we are measuring (e.g., temperature, distance, height.) Observations are repeated measurements of a variable on different experimental units. Structuring your data following this rule will put you in a great position for processing and analyzing your data in an automated way. First, using a standard approach to formatting data, where the structure of a table reflects the meaning of its content (i.e., where a column always means a variable and a row always means an observation) removes any ambiguity for yourself and for the computer. Second, this specific approach works very well with programming languages, like R, that support vector calculations (i.e., calculations on entire columns or tables at a time, instead of single elements only.) 4.4 Problematic practices In practice, what are some common habits that contradict the principles of tidy data? Here are some examples. 4.4.1 Multiple variables in a single column Putting information for two separate variables into one column contradicts the “one column per variable” principle. A classic example of this is merging age class and sex into one – for example “adult female”, “juvenile male”, etc. If, for instance, later on we wanted to filter based on age only, we would have to first separate the information within the column and then apply a filter. It is much more straightforward to combine information than to split it, so the structure of our data should always follow a reductionist approach where each table unit (cell) contains a single data unit (value). img 4.4.2 Rows for variables, columns for observation While the choice of using rows for observations and columns for variables may seem arbitrary – after all, why can’t it be the other way around? – consider this: the most commonly used data structure in R, the data frame, is a collection of vectors stored as columns. A property of vectors is that they contain data of a single type (for instance, you can’t have both numbers and characters in the same vector, they either have to be all numbers or all characters). Now imagine a situation where a dataset includes weight measurements of individual green frogs captured in different ponds: for each frog, we’d have a weight value (a number) and the name of the pond where it was captured (a character string). If each frog gets a row, we get a column for weight (all numbers) and a column for pond (all characters). If each frog gets a column and weight and pond go in different rows, each column would contain a number and a character, which is not compatible with R. This is valid not only for R, but for other vector-based programming languages too. The tidy data format makes sure your data integrates well with your analysis software which means less work for you getting the data cleaned and ready. img 4.4.3 Multiple tables in a single spreadsheet Creating multiple tables in a single spreadsheet is problematic because, while a human can see the layout (empty cells to visually separate different tables, borders, etc.) and interpret the tables as separate, the computer doesn’t have eyes and won’t understand that these are separate. Two values on the same row will be interpreted as belonging to the same experimental unit. Having multiple tables within a single spreadsheet draws false associations between values in the data. Starting from this format will invariably require some manual steps to get the data into a format the computer will read. Not good for reproducibility! img 4.4.4 Multiple sheets This one may seem innocuous, but actually it can be problematic as well. For starters, you can’t load multiple sheets into R at the same time (by multiple sheets, I mean the tabs at the bottom). If you’re only using base R functions, you can’t even load a .xslx file (although you can using packages such as readxl), so you’re going to have to save your spreadsheets as .csv before importing. When saving as .csv, you’ll only be able to save one sheet at a time. If you’re not aware of this, you end up losing track of the data that was in the other sheets. Even if you are aware, it just makes it more work for you to save each of the sheets separately. Using multiple sheets becomes even more of a problem when you are saving the same type of information into separate sheets, like for example the same type of data collected during different surveys or years. This contradicts another principle of tidy data, which is each type of observational unit forms a table. There’s no reason to split a table into multiple ones if they all contain the same type of observational unit (e.g., morphometric measurements of frogs from different ponds). Instead, you can just add an extra column for the survey number or year. This way you avoid inadvertently introducing inconsistencies in format when entering data, and you save yourself the work of having to merge multiple sheets into one when you start processing and analyzing. img 4.4.5 Using formatting to convey information Anything that has to do with visual formatting is not computer-readable. This includes borders, merging cells, colors, etc. When you load the data into an analysis software, all the graphical features are lost and all that is left is… rows and columns. Resisting the temptation to merge cells, and instead repeating the value across all rows/columns that it applies to, is making your data computer-friendly. Resisting the urge to color-code your data, and instead adding an additional column to encode the information you want to convey with the different colors, is making your data computer-friendly. Columns are cheap and there’s no such thing as too many of them. img 4.4.6 Putting units in cells Ideally, each measurement of a variable should be recorded in the same units. In this case, you can add the unit to the column name. But even if a column includes measurements in different units, these units should never go after the values in the cells. Adding units will make your processing/analysis software read that column as a character rather than as a numeric variable. Instead, if you need to specify which unit each measurement was taken in, add a new column called “variable_unit” and report it there. Remember, columns are cheap! img 4.4.7 Using problematic column names Problematic column names are a lot like the problematic file names from Chapter 1. They are non-descriptive, they contain spaces or special characters – in short, they are human-readable but not computer-readable. Whether you separate words in column names using camel case or underscores, avoid using spaces. It’s a good idea to include units in the column names, e.g., “weight_g”, “distance_km”, etc. Also, just like for file names, be consistent in your capitalization pattern and choice of delimiters. img 4.4.8 Conflating zeros and missing values Conflating zero measurements with missing values by using a zero or a blank cell as interchangeable is a problem, because there’s a big difference between something you didn’t measure and something that you did measure and it was zero. Any blank cell will be interpreted as missing data by your processing/analysis software, so if something is zero it needs to be actually entered as a zero, not just left blank. Similarly, never use zero as your value for missing data. These are two separate meanings that need to be encoded with different values. 4.4.9 Using problematic null values Many common ways to encode missing values are problematic because processing/analysis software does not interpret them correctly. For example, “999” or “-999” is a common choice to signify missing values. However, computers are very literal, and those are numbers. You may end up accidentally including those numbers in your calculations without realizing because your software did not recognize them as missing values. Similarly, like we said, “0” is indistinguishable from a true zero and should never be used to signify “missing data”. Worded options, such as “Unknown”, “Missing”, or “No data” should be avoided because including a character value into a column that is otherwise numeric will cause the entire column to be read as character by R, and you won’t be able to do math on the numbers. Using the native missing value encoding from your most used programming language (e.g., NA for R, NULL for SQL, etc.) is a good option, although it can also be interpreted as text in some instances. The recommended way to go is to simply leave missing values blank. The downside to this is that, while you’re entering data, it can be tricky to remember which cells you left blank because the data is missing and which ones are blank because you haven’t filled them yet. Also, if you accidentally enter a space in an empty cell, it will look like it’s blank when it actually is not. 4.4.9.1 Inconsistent value formatting A very common problem arises when having to filter/tally/organize data based on a criterion that was recorded inconsistently in the data. I find this to be especially common for columns containing character-type variables. For example, if the person entering data used “F” or “Female” or “f” interchangeably in the “sex” column, it will be difficult later on to know how many female individuals are in the dataset. Similarly, if the column “observer” contains the name of the same person written in multiple different ways (e.g., “Mary Brown”, “Mary Jean Brown”, “M. Brown”, “MJ Brown”), these won’t be recognized as the same person unless these inconsistencies are fixed later on. 4.5 Document, document, document! If you follow all the guidelines outlined in this Chapter, chances are you will be able to automate all of your data processing without having to do anything manually, because the data will be in a computer-readable format to begin with. However, if you do end up having to do some manual processing, make sure you thoroughly document every step. Like we discussed in Chapter 1, never edit your raw data; save processed/cleaned versions as new files; and describe all the steps you took to get from the raw data to the clean data in your README file. 4.6 References https://datacarpentry.org/spreadsheet-ecology-lesson Wickham, Hadley. “Tidy data.” Journal of Statistical Software 59.10 (2014): 1-23. White, Ethan P., et al. “Nine simple ways to make it easier to (re) use your data.” Ideas in Ecology and Evolution 6.2 (2013). "],
["relational-databases.html", "Chapter 5 Relational Databases 5.1 What is a relational database? 5.2 Why bothering with relational databases? 5.3 Database components 5.4 Database design and architecture 5.5 Data types 5.6 A first look at our practice database", " Chapter 5 Relational Databases 5.1 What is a relational database? A relational database is a collection of interrelated tables. The relationships between tables represent actual relationships between data entities. The relationships between data entities always exist in real life, no matter our choice of data management system; but they are only made explicit to the computer when the data are stored in a relational database. For example, say that we collected data in the field for green frogs where, the first time an individual is captured, we record its sex, take some biometric measurements, and assign it an ID. Then, the animals are recaptured periodically to take new biometric measurements. We’d store our data in two spreadsheets, one that has a list of all the frogs we’ve ever captured with their ID and sex. The second spreadsheet has all the biometric measurements, and each frog ID may appear more than once. The two spreadsheets are related because they both contain information about the same individuals: the first one contains one-time information, and the second one contains repeated records for each individual. However, the computer does not know about this relationship, only we do. By using a relational database we translate real-world relationships between data entities into structural relationships between our data tables. 5.2 Why bothering with relational databases? You may ask, why should we care to enforce structural relationships between tables? Isn’t it enough to just know how things are related? Well, the first advantage of relational databases is that they force us to think critically about the real-life relationships between data entities, which we often take for granted and we might overlook. Many problems with data inconsistencies directly stem from not having a robust logical structure to our data organization. Enforcing a logical data structure goes hand in hand with quality control and assurance. By designing a mental map of what the different pieces of information look like and how they relate to one another, we can easily spot conditions that need to be verified in order for the data to make sense. If any of those conditions aren’t verified, something isn’t right. For example, imagine having a table of capture and mortality dates for the frogs in addition to biometric measurements. It is logically impossible for any of the biometrics measurements of an individual to be taken outside of the interval between first capture and death. If we store our data in spreadsheets, we may not even notice if there’s a record supposedly taken before an individual was captured or after it died. But if we make the relationship between the two tables explicit, the computer will recognize impossible situations like these for us. Another advantage of relational databases is that they allow for optimal data storage by avoiding redundancy of information across tables. There is no need to repeat the same data across multiple tables because the relationships between tables allow us to cross-link data across them. For example, there is no need to repeat the sex and capture date of an individual frog for each biometrics measurement we take of that individual. As long as we can relate the two tables based on the individual ID, we can always retrieve and combine information across them. This improves accuracy by avoiding duplication and it reduces the space our data occupies on our computer. There are several more advantages to using relational databases, including fast (like, extremely fast) processing over large amounts of data. If the database is stored on an external server, that also helps managing data across collaborators within a project by having a centralized repository that everybody can connect to and use, which means everyone always has the same and most up-to-date version of the data. It’s also possible to set up user profiles with different privileges, like read-and-write or read-only. Storing the database on an external server also means that data processing and computation don’t happen on your local computer and thus they don’t take up your memory, which is often limited. Finally, concepts of relational database management are key to understanding data manipulation in other languages. 5.3 Database components The core unit of a relational database is a table: a two-dimensional structure composed by rows (or records), and columns (or fields). Each row is an observation and each column is a variable (sounds familiar? We talked about tidy data in 4.3) Tables relate to one another through foreign keys. A foreign key is a variable that is shared between two tables. For instance, in the frogs example above, the individual ID is the variable relating each biometric measurement to the static features of each individual (sex, first capture date, mortality date.) Foreign keys are what makes relational databases “relational”. A table can have one or more foreign keys, each relating it to another table. In addition to foreign keys, each table in a relational database also has a primary key. A primary key is a unique identifier for each record in the table. Sometimes one variable lends itself well to being a primary key, for example the individual ID would be a suitable primary key for the table of static individual information, because each individual appears only once and thus the ID is a unique identifier of rows. However, the individual ID would not be a suitable primary key for the table of biometric measurements, because IDs are repeated and therefore not unique. When there isn’t a suitable variable to serve as a primary key, we can simply add a serial number that uniquely identifies each record. 5.4 Database design and architecture The structure of a database should reflect real-world structure in the data. The way we split data into tables and the relationships we enforce between them should mirror the real way data entities relate to one another. Designing a database has little to do with software and coding and everything to do with thinking about the real structure in our data and how can we best represent it using the building blocks of a database. In general, tables should we split based on the sampling unit of the data they contain. For example, data on individual frogs and data on the ponds where the frogs came from should be stored in separate tables. One table has the frog as its unit and one has the pond. These two tables could then be put in relation based on the pond each frog was caught in. Two tables may still deserve to be split even if they refer to the same real-world sampling unit. The example we made at the beginning where we have a table of individual ID and sex and a table of individual biometric measurements is a good illustration for this. In both of those tables, the sampling unit is an individual frog, but the first table contains static information that does not change through time, whereas the second table contains dynamic information, which translates into repeated measures for each individual. Instead of keeping these data together into a single table and repeating the static information at each of the repeated individual records, it is much more efficient to split the static and dynamic information. We can always put the two tables in relation based on the frog ID. A database should also be designed so that each column only contains atomic values, which means values that cannot be further divided. This means, for example, separating first and last name into two columns instead of a “full name” column (not talking about frogs here.) It also means lists of several items are not allowed within one cell. Whenever we find ourselves in a situation where we need to enter a list of items inside a single cell, it means that the structure of the current table is not optimized and we need to rethink it. For example, let’s say that we are collecting data on frog diet. Each diet sample may contain multiple food items, e.g., a fly, an ant, a beetle, a cricket. If our diet table uses the diet sample as a unit (one sample per row), we’ll end up with a list of multiple entries in the “food item” column, which would no longer be atomic. A good alternative would be to use the food item as the unit of the table instead. We can have one food item per row and repeat the ID of the diet sample for each item that was found in that sample. 5.5 Data types Data types define what kind of data is stored in each column of a table. Different database systems recognize different sets of data types, but the most fundamental are common to many. These include character strings, numerical values (and special cases of these, such as integers), boolean values (TRUE or FALSE), etc. Because each column in a table has a data type associated to it, it means that you can’t mix data types inside the same column. You can’t, for example, use both “5” (for 5-year-old) and “adult” in an “age” column. 5.6 A first look at our practice database In the next chapter, we are going to take a deep dive into SQL, the most used programming language for relational databases. We are going to learn how to write queries to retrieve data and how to build the database itself. The database we’ll practice with includes imaginary data on dragons. Let’s take a look at the content and structure: "],
["sql.html", "Chapter 6 Basics of SQL Language 6.1 The SQL language 6.2 Writing SQL queries 6.3 Building a database", " Chapter 6 Basics of SQL Language 6.1 The SQL language The acronym SQL stands for Structured Query Language. It is the most used language worldwide for relational database management systems. Whenever you interact with a database in your daily life (for example, any time you browse an online shopping website, or any time you look up something on Google), you can bet that it was written in SQL. SQL is a language that is used and understood by different database management systems, such as Oracle, PostgreSQL, MySQL, or SQLite. While all these programs share many similarities, which one of these you’ll end up using depends on the specifics of what you need it for. For the purposes of this class, we’ll be using SQLite. SQLite provides all the core functionalities you’ll need to get familiar with databases while being lightweight in terms of setup (easy to install and to configure). At the end of this chapter, we’ll talk about why you may want to make a different choice in the future, but for now we’ll stick with SQLite. Each database management program uses a slightly different dialect of SQL. The language is the same, but there may be some slight differences in the syntax between different programs. I’ll point out throughout the chapter when we encounter syntax that is specific to the SQLite dialect. 6.2 Writing SQL queries The nice thing about SQL is that it sounds a lot like human language. An SQL statement, or query, always begins with a verb that describes the action we want to do. A semicolon at the end of the query lets SQL know that we are done talking. For example: SELECT dragon_id FROM dragons; The verb SELECT is one we’ll be using over and over again. Any time you want to select data you also have to specify which table you want it from, so the SELECT clause is always followed by a FROM clause. In this case, we asked to see the column dragon_id from the dragons table. Sounds like English, right? If we want to see multiple columns, we can list them separated by commas: SELECT dragon_id, sex FROM dragons; And if we want to see the whole table (not specific columns), we can use a wildcard: SELECT * FROM dragons; 6.2.1 Limiting results If the whole table is too big and we only want to take a look at the first, say, 10 rows, we can add a LIMIT clause: SELECT * FROM dragons LIMIT 10; 6.2.2 Sorting results One thing to remember with SQL is that the data returned by a SELECT statement does not necessarily return the data in any specific order unless we tell it to. You can never assume the output is already sorted. To be explicit about how you want the data ordered, you can add an ORDER BY clause: SELECT * FROM dragons ORDER BY species LIMIT 10; The default ordering will be ascending. If we want descending ordering instead, we can specify that as follows: SELECT * FROM dragons ORDER BY species DESC LIMIT 10; 6.2.3 Finding unique values Say that we wanted to know what species of dragons are in the database. We can ask for unique values as follows: SELECT DISTINCT species FROM dragons; 6.2.4 Filtering We can filter data based on any logical condition using a WHERE clause. For instance, say that we we only want Norwegian Ridgebacks: SELECT * FROM dragons WHERE species = &#39;Norwegian Ridgeback&#39;; We can specify multiple conditions at once. For example, if we want only female Norwegian Ridgebacks: SELECT * FROM dragons WHERE species = &#39;Norwegian Ridgeback&#39; AND sex = &#39;F&#39;; In the example above, both conditions have to be satisfied at once – dragons have to be Norwegian Ridgeback and females to appear in the results. What if we want to return data where at least one of two conditions is satisfied? The following query will return dragons that are either Norwegian Ridgebacks or Peruvian Vipertooths: SELECT * FROM dragons WHERE species = &#39;Norwegian Ridgeback&#39; OR species = &#39;Peruvian Vipertooth&#39;; A more concise way to write the same query would be: SELECT * FROM dragons WHERE species IN (&#39;Norwegian Ridgeback&#39;, &#39;Peruvian Vipertooth&#39;); Logical conditions also include “greater than”, “less than”, “not equal to”. For instance, we can exclude Norwegian Ridgeback like so: SELECT * FROM dragons WHERE species != &#39;Norwegian Ridgeback&#39;; Let’s see some examples with numbers using the morphometrics table. The following query returns any dragons with wingspan greater than 8 meters (incidentally, you can include comments in SQL is by using a double dash): SELECT * FROM morphometrics WHERE wingspan_cm &gt; 800; -- 800 cm is 8 meters We can also sort the output of the filter: SELECT * FROM morphometrics WHERE wingspan_cm &gt; 800 ORDER BY wingspan_cm DESC; 6.2.5 Calculations The morphometric measurements are in centimeters, but we often define our filtering conditions using meters in our minds (see above, “wingspan greater than 8 meters”). Instead of converting the condition into centimeters, wouldn’t it be easier to have SQLite transform the data in meters and then evaluate the condition? We can do this calculation on the fly: SELECT * FROM morphometrics WHERE wingspan_cm/100 &gt; 8; 6.2.6 Aggregate functions Summary statistics are calculated using aggregate functions in SQL. Counting, summing, calculating the mean, minimum, maximum values are all examples of aggregate functions. Let’s see how they work. If we want to know how many Norwegian Ridgebacks are in the database, we can use: SELECT COUNT(*) FROM dragons WHERE species = &#39;Norwegian Ridgeback&#39;; The dragons table has no repeated IDs, so we shouldn’t have counted anybody twice. However, let’s double check if that’s true by specifying that we want to only count distinct IDs: SELECT COUNT(DISTINCT dragon_id) FROM dragons WHERE species = &#39;Norwegian Ridgeback&#39;; Now let’s see how to calculate some summary stats on dragon morphometrics. What is the mean total body length of dragons in our sample? SELECT AVG(total_body_length_cm) FROM morphometrics; We can also calculate several different things at once: SELECT AVG(total_body_length_cm), MIN(total_body_length_cm), MAX(total_body_length_cm) FROM morphometrics; 6.2.7 Aliases Aliases are temporary names that we can assign to a column (or a table) during the execution of a query. For example, we can use an alias to rename the output of our mean body length calculation: SELECT AVG(total_body_length_cm) AS mean_body_length FROM morphometrics; 6.2.8 Grouping If we want to apply the same calculation to different groups within the data, we can use the GROUP BY clause. GROUP BY is used in conjunction with an aggregate function to return the computed values broken down by group. For instance, if we want to count how many dragons we have for each species (and sort them from the most numerous to the least numerous): SELECT species, COUNT(*) AS n_dragons FROM dragons GROUP BY species ORDER BY n_dragons DESC; 6.2.9 Filtering based on computed values If we want to apply any filtering to our results based on calculated values, the WHERE clause is not going to work. Instead, a HAVING clause is used in combination with an aggregate function and a GROUP BY. HAVING does the same thing as WHERE except that it handles logical conditions on computed values. For example, say that we want to count how many individuals we have in our database and then only keep the species for which we have at least 50 individuals. The following query counts how many individual dragons we have for each species: SELECT COUNT(DISTINCT dragon_id) AS n_dragons, species FROM dragons GROUP BY species; Because the count is a calculated value (the output of the aggregate function COUNT), if we try to apply a filter using WHERE we’ll get an error: SELECT COUNT(DISTINCT dragon_id) AS n_dragons, species FROM dragons GROUP BY species WHERE n_dragons &gt; 50; Instead, we can use HAVING: SELECT COUNT(DISTINCT dragon_id) AS n_dragons, species FROM dragons GROUP BY species HAVING n_dragons &gt; 50; 6.2.10 Null values Missing values in SQL are represented as NULL. We can filter (or filter out) these values using the IS NULL operator: SELECT * FROM dragons WHERE sex IS NULL; The query above returns all the dragons for which sex is unknown. If we want to exclude any individuals for which sex is unknown, then we can add the NOT operator to our statement: SELECT * FROM dragons WHERE sex IS NOT NULL; 6.2.11 Joins So far, we have learned how to query data from a single table. But what if we need to combine information from multiple tables to get the result we want? Let’s make an example. Say that we want to calculate the average wingspan for male versus female dragons. The wingspan measurements are in the morphometrics table. However, the morphometrics table does not have a “sex” column. The information on sex is in the dragons table. To get our answer, we need to combine information from the dragons and morphometrics tables. To do so, we introduce SQL joins. Joins are the heart of relational database use. Without joins, there is no point in a relational database because we can’t take advantage of the relations between tables. Joins exploit the links we established between tables via foreign keys to combine information across them. There are several types of join. The most common types are left join, inner join, or full join. To understand this terminology, consider this: whenever you are joining two tables, the first table you mention (the one to which you’re joining) is called the left table, whereas the second table (the one you’re joining to the first) is called the right table. With a left join, you keep all the records in the left table and add information from the right table whenever there’s a matching row. A full join means that you retain all rows from both tables, matching them whenever possible. An inner join means that you only retain the rows that match between the two tables. img Let’s look at this in practice and it will make more sense. Our database contains a deployments table and a morphometrics table. Not all dragons that were tracked were also measured. If we do a left join using the deployments as the left table, we get in output the whole deployments table (all the dragons that were tracked) with the associated morphometric information whenever available (for the dragons that were also measured): SELECT * FROM deployments -- this is our left table LEFT JOIN morphometrics -- this is our right table ON deployments.dragon_id = morphometrics.dragon_id -- this is the shared column LIMIT 10; Since the shared column is named the same in the two tables (dragon_id), I had to specify which table I was referring to each time in the ON clause, or it would have been ambiguous. The syntax to do so is table.column. Also note that, because the dragon ID column appears in both tables and we did not specify which table we wanted it from, SQLite is duplicating it. To avoid that, we need to get rid of the wildcard and spell out the name of each of the columns we want in output (also specifying the table when ambiguous): SELECT deployments.dragon_id, date, tag_id, start_deployment, end_deployment, total_body_length_cm, wingspan_cm, tail_length_cm, tarsus_length_cm, claw_length_cm FROM deployments LEFT JOIN morphometrics ON deployments.dragon_id = morphometrics.dragon_id LIMIT 10; If we invert the tables (the morphometrics becomes the left table and the deployments becomes the right), we get in output the whole morphometric table (all the dragons that were measured) with the associated deployment information whenever available (for the dragons that were also tracked): SELECT * FROM morphometrics -- this is our left table LEFT JOIN deployments -- this is our right table ON morphometrics.dragon_id = deployments.dragon_id LIMIT 10; An inner join only keeps the rows that match between two tables. In this case, that means we only get in output data for animals that were both tracked and measured: SELECT * FROM morphometrics INNER JOIN deployments ON morphometrics.dragon_id = deployments.dragon_id LIMIT 10; A full join keeps all records from both the left and the right table, matching them whenever possible. That is, we get in output all the dragons, with blanks in the morphometrics table for those that were only tracked and blanks in the deployments table for those that were only measured. Unfortunately, SQLite does not support full joins, so we cannot demonstrate it here, but it’s important to know that a full join exists and what it does because you’ll find yourself using it in any other database management program or even in R (see Chapter 13). 6.2.12 Nested SELECT statements Now that we know how to use joins, we can go back to our challenge of calculating the average wingspan for male versus female dragons. What we need to do is add the sex column to the morphometrics table. We can do this by looking at the dragon ID, which is the column the two tables have in common. So the join clause will look like this: SELECT dragon_id, wingspan_cm, sex FROM morphometrics -- this is our left table LEFT JOIN dragons -- this is our left table ON morphometrics.dragon_id = dragons.dragon_id; -- this is the shared column By using a left join, we are saying that we want to keep all rows from the left table (morphometrics) while adding information on sex whenever available. Now we can compute the average wingspan broken down by sex: SELECT sex, AVG(wingspan_cm) AS mean_wingspan FROM ( SELECT morphometrics.dragon_id, wingspan_cm, sex FROM morphometrics LEFT JOIN dragons ON morphometrics.dragon_id = dragons.dragon_id ) GROUP BY sex; The one above is a nested query, which means there are two SELECT statements nested within one another. The inner SELECT statement (everything between the parentheses) is treated as if it were a table in the database. SQL will execute the query from the inside out, so even though that table physically does not exist, it gets temporarily created when the inner query is run and is then used in input to the second query. 6.2.13 Order of operations The fact that SQL will execute nested queries from the inside out is one aspect of order of operations. But there’s also a logic to the order in which SQL executes clauses within a query: first, it gathers the data, then it filters it and aggregates it as needed, then it computes results, and finally it sorts them and truncate them if necessary. Let’s look at this step by step: First, SQL executes the FROM clause and any JOIN clauses, if present, to determine what tables are being queried. Then, it will execute the WHERE clause to filter the data. Then, it will GROUP BY the column/s of interest. Then it will calculate any derived values based on aggregate functions. Then, it will apply any filters specified by HAVING. Now, and only now, it will execute the SELECT clause! Once the computations are completed and the output of SELECT is ready, SQL can start refining it. First, it will discard any duplicates if DISTINCT is specified. Then, it will sort the results based on ORDER BY. Finally, it will truncate the output if a LIMIT is specified. If you think about it, all of this makes logical sense. You can’t select a column if you’re not sure what table to look in. You can’t compute a per-group count before defining the groups. You can’t order results that you haven’t computed yet. But then why do we start our queries with SELECT? And does the order in which we write our queries matter? The answer to both of those questions is that there is a difference between the logical order of operations and the lexical (or syntactical) order of operations, which is how we actually write queries. Even though these don’t correspond, writing a query in any other order than the following is incorrect (and will return an error): SELECT FROM JOIN ON WHERE GROUP BY HAVING ORDER BY LIMIT; 6.3 Building a database 6.3.1 Creating a new database in SQLite Studio We are now going to see how to build a database by recreating the dragons database we have been practicing on. Let’s open up SQLite Studio and, on the Database tab, click “Add a database” (or use Ctrl/Cmd + O as a shortcut). Click on the green plus sign, navigate to the folder where you want to save the database, and choose a file name. Click “Save”. Now we need to also choose a name to use internally (this name will appear in the drop-down list of databases within SQLite, but it does not necessarily need to be the same as the filename.) Click “OK” and you should see your new database in the list on the left. 6.3.2 Creating tables Now let’s open the SQL editor (Tools &gt; SQL editor). Double check that you are working on the correct database (the name of the active database is shown in the toolbar). We are ready to create our first table! To create a table, we need to specify a name for the table, as well as a name and a data type for each column. The basic format is as follows: CREATE TABLE table_name ( column_1 data_type, column_2 data_type, column_3 data_type, ... ) Let’s start by creating the dragons table. Remember that each table needs to have a primary key, i.e., a column that serves as the unique identifier of each row. The values in this column need to be unique and cannot be null. In this table, we can use the dragon ID as the primary key: CREATE TABLE dragons ( dragon_id varchar(5), sex char(1), age_class varchar(8), species varchar(50), PRIMARY KEY (dragon_id) ); Notice that I specified a number of digits for each character variable. Because the format of the dragon ID is “D” + number from 1 up to 500 (for now), setting the ID as a character string with varying size up to 5 digits allows for IDs from “D1” to “D9999”. So far, we only have 500 individuals, so strictly it would have been enough to set the limit to 4 digits only, but it’s good to have some forethought and allow for growth into the future. At the same time, it seems reasonable to assume we won’t catch 10 thousand dragons in this project, so 5 digits is a good compromise. In reality, space is rarely limiting to the point where 4 or 10 digits makes a difference, so when in doubt err on the side of more room for growth. While dragon ID, age class, and species have a variable number of digits, sex always has one (it can be “M” or “F”). So we can make this a char instead of a varchar and specify it will have 1 digit. 6.3.3 Adding constraints We can also add some constraints for the purpose of quality assurance. For example, because the dragon ID is the primary key, we can’t accept null values in this column. Sex is always one of “M” or “F” and age is always one of “Juvenile”, “Subadult”, or “Adult”. In other SQL dialects you can add these after the fact, but in SQLite the only way to add these constraints is at the same time as you create the table. We can delete the table and re-create it by modifying the above syntax as follows: DROP TABLE dragons; CREATE TABLE dragons ( dragon_id varchar(5) NOT NULL, sex char(1) CHECK (sex IN (&#39;M&#39;, &#39;F&#39;)), age_class varchar(8) CHECK (age_class IN (&#39;Juvenile&#39;, &#39;Subadult&#39;, &#39;Adult&#39;)), species varchar(50), PRIMARY KEY (dragon_id) ); 6.3.4 Order of table creation Foreign keys are also specified as constraints. Because it’s not possible to enforce constraints as an afterthought, we need to plan the order in which we add tables carefully. If we try to add a foreign key that refers to a table that doesn’t exist yet, we won’t be able to. Let’s take another look at the diagram of our table relationships: img Any table that has one or more foreign keys needs to be added after the related table/s. In our case, there are three “root” tables that do not have any foreign keys: the dragons table, the tags table, and the capture sites table. We can add these three first and then all the others. The tags table has a tag_id field which is unique and can therefore be used as a primary key. Note that when you make a column the primary key you automatically enforce a unique constraint on that column. However, SQLite does not enforce a not-null constraint on primary keys like most other SQL dialects. Because SQLite doesn’t, we have to do it ourselves: CREATE TABLE tags ( tag_id char(6) NOT NULL PRIMARY KEY, brand varchar(50), status varchar(20) ); The capture sites table has a site field which contains a 3-letter code that uniquely identifies the site. We can use that as a primary key. The UTM coordinates are numeric values for which we can use the data type called “double precision”, or just “double”. There are several possible choices for numeric data types. “Double precision” and “float” are both suitable data types for storing real numbers, but double precision stores numeric values with higher precision than float. Both of these allow for variable decimal places, whereas the data type “decimal” enforces the same number of decimal places across records. The choice between these data types depends on 1. whether we want the measurements to all have the same number of decimal places or not, and 2. how many significant digits do we expect/care about. Double precision can store up to 15 significant digits (unlike float which can store up to 8), but it also occupies double the space (64 bit instead of 32.) For UTM coordinates, the Y value has a minimum of 7 digits, and we want to keep decimal places because we need the highest precision possible on the position of animals in space. Therefore, we’ll go with double precision: CREATE TABLE capture_sites ( site char(3) NOT NULL PRIMARY KEY, utm_x double, utm_y double ); Note that I used an alternative syntax to specify the primary key. This is equivalent to the one I used above for the dragons table. 6.3.5 Populating the tables Now that we added a few tables, we can start to populate the database by loading the data as .csv files. On the Tools tab, click “Import.” img Check that the database is correct and then select the table you want to import data into. For example, let’s start with the dragons table: img Click Next. Now, browse your directory to select the input file. Then check the box that says “First line represents CSV column names”, make sure the field separator is a comma, and set the NULL values to NA (which is how they are encoded in the .csv.) Click Finish. To check if the table was imported correctly, go back into the query editor and try to look at the first 10 rows: SELECT * FROM dragons LIMIT 10; If everything looks good, we can go ahead and import data in the other two tables. 6.3.6 Autoincrements as primary keys Sometimes a table does not contain any columns with unique values. This is the case for all the tables we have left to add. What do we use as primary key in these situations? Adding a column with a serial number that gets automatically updated is a good option. SQLite has something called auto-increment, which is an integer column that automatically increases by 1 each time you add a row to a table. Because it’s incremental, the values of an auto-increment will always be unique, and because it’s automatic, they will never be null. Sounds like a perfect candidate for a primary key. The problem with using an auto-increment as a primary key arises when you’re trying to import data from a .csv file. If the column with the auto-increment does not already exist in the .csv, SQLite won’t let you import the file into the table because the number of columns does not match. We don’t want to add the auto-increment into the .csv ourselves because that defeats the purpose of having SQLite doing it for us. But if we import the .csv as it is (without primary key) and then add the auto-increment column later, we won’t be able to make it the primary key because SQLite won’t let us. So, what do we do? One workaround that people have found to solve this issue is to trick SQLite by using a temporary table. This is how the process works: Create the table the way we want it, with an auto-increment primary key plus all the columns we want to import from the .csv; Create a temporary table without primary key that only contains the columns from the .csv; Import the .csv into the temporary table; Populate the final table by pulling data from the temporary table; Delete the temporary table. A little convoluted, but it does the job. Let’s demonstrate this on the captures table. First, we create the table like we want it to look in the end (note that I am adding foreign keys; we’ll go over that part in the next section): CREATE TABLE captures ( capture_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), date text, site char(3), FOREIGN KEY(dragon_id) REFERENCES dragons(dragon_id) FOREIGN KEY(site) REFERENCES capture_sites(site) ); Second, we create a temporary table without the primary key (no need to add foreign keys to this one as we are going to delete it anyway): CREATE TABLE captures_temp ( dragon_id varchar(5), date text, site char(3)); Now on Tools &gt; Import, we upload captures.csv into captures_temp. Then we can populate the final table as follows: INSERT INTO captures(dragon_id, date, site) SELECT * FROM captures_temp; And finally we delete our temporary table: DROP TABLE captures_temp; We better get familiar with this workflow because we are going to use it for all the other tables now. 6.3.7 Foreign keys For each of the remaining tables, we will specify one or more foreign keys to enforce the relationships between tables. Each foreign key is the primary key of another table. See for example what we did above: the captures table we just imported contains information on when and where each dragon was captured. This means this table needs to have two foreign keys: the dragon_id column links it to the dragons table and the site column links it to the capture sites table. Now let’s apply the concept to the other tables. The morphometrics table will have a single foreign key linking it to the dragons table (the dragon ID). The second column is a date – SQLite does not have a dedicated data type for dates. Instead, we stored this as a character string in ISO8601 format (“YYYY-MM-DD HH:MM:SS.SSS”). This time there is no need for double precision because we want to only retain up to the third decimal place and the numbers are not larger than ~3000, so 8 significant digits is sufficient. We’ll use float as the data type for the measurements. Because individuals may have been measured multiple times, none of the existing columns are unique. This means we’ll create a serial number to use as the primary key. We’ll use the same trick as above: CREATE TABLE morphometrics ( measurement_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), date text, total_body_length_cm float, wingspan_cm float, tail_length_cm float, tarsus_length_cm float, claw_length_cm float, FOREIGN KEY (dragon_id) REFERENCES dragons(dragon_id) ); CREATE TABLE morphometrics_temp ( dragon_id varchar(5), date text, total_body_length_cm float, wingspan_cm float, tail_length_cm float, tarsus_length_cm float, claw_length_cm float ); Now on Tools &gt; Import, we upload morphometrics.csv into morphometrics_temp. Then we populate the final table: INSERT INTO morphometrics(dragon_id, date, total_body_length_cm, wingspan_cm, tail_length_cm, tarsus_length_cm, claw_length_cm) SELECT * FROM morphometrics_temp; And delete our temporary table: DROP TABLE morphometrics_temp; The diet table contains repeated sample IDs and repeated item IDs within each sample. We’ll need a serial number here too because none of the columns are unique. The item ID is an integer so we are going to use a new numeric data type for it. The foreign key will be, again, the dragon ID referring to the dragons table: CREATE TABLE diet ( diet_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), sample_id varchar(8), date text, item_id integer, item varchar(50), FOREIGN KEY (dragon_id) REFERENCES dragons(dragon_id) ); CREATE TABLE diet_temp ( dragon_id varchar(5), sample_id varchar(8), date text, item_id integer, item varchar(50) ); Upload diet.csv into diet_temp. INSERT INTO diet(dragon_id, sample_id, date, item_id, item) SELECT * FROM diet_temp; DROP TABLE diet_temp; The deployments table assigns a tag to each individual within a certain period of time. The dragon_id column will be the foreign key that links it to the dragons table, and the tag_id column will link it to the tags table. The start and end deployment dates will be stored as ISO8601 text. Again, we’ll need a serial number to use as a primary key: CREATE TABLE deployments ( deployment_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), tag_id char(6), start_deployment text, end_deployment text, FOREIGN KEY(dragon_id) REFERENCES dragons(dragon_id) FOREIGN KEY(tag_id) REFERENCES tags(tag_id) ); CREATE TABLE deployments_temp ( dragon_id varchar(5), tag_id char(6), start_deployment text, end_deployment text ); Upload deployments.csv into deployments_temp. INSERT INTO deployments(dragon_id, tag_id, start_deployment, end_deployment) SELECT * FROM deployments_temp; DROP TABLE deployments_temp; Now we can input the telemetry data. This table contains the raw tracking data as we download it from the tags. We’ll need a serial number to uniquely identify each record, and we’ll add the tag ID as the foreign key to the tags table: CREATE TABLE gps_data_raw ( gps_id INTEGER PRIMARY KEY AUTOINCREMENT, tag_id char(6), timestamp text, utm_x double, utm_y double, FOREIGN KEY(tag_id) REFERENCES tags(tag_id) ); CREATE TABLE gps_data_raw_temp ( tag_id char(6), timestamp text, utm_x double, utm_y double ); Upload telemetry_raw.csv into gps_data_raw_temp. INSERT INTO gps_data_raw(tag_id, timestamp, utm_x, utm_y) SELECT * FROM gps_data_raw_temp; DROP TABLE gps_data_raw_temp; 6.3.8 Crossing existing information to derive new tables The raw GPS data table does not give us any information about which animal each location corresponds to; all we know is the tag ID. Each tag does not correspond to an individual, because some tags are reused on multiple dragons. So how do we make these data usable? How do we know who is who? To associate each location to the correct animal, we need to know who was wearing the tag at the time that location was taken. There is a very elegant solution to our problem, as we can pull the range of dates an individual was wearing a certain tag from the deployments table, cross those dates with the dates in the raw GPS data, and create an updated telemetry table where each location is assigned to the correct individual. No manual work involved. Are you ready for some magic?! First, we create the table structure: CREATE TABLE gps_data ( loc_id INTEGER PRIMARY KEY, tag_id char(6), dragon_id varchar(5), timestamp text, utm_x double, utm_y double, FOREIGN KEY (tag_id) REFERENCES tags(tag_id) FOREIGN KEY (dragon_id) REFERENCES dragons(dragon_id) ); And then we populate it by pulling information from the raw GPS and deployment tables. Locations are assigned to the individual that was wearing the tag at the time based on the WHERE clause: INSERT INTO gps_data ( tag_id, dragon_id, timestamp, utm_x, utm_y) SELECT deployments.tag_id, deployments.dragon_id, gps_data_raw.timestamp, gps_data_raw.utm_x, gps_data_raw.utm_y FROM deployments LEFT JOIN gps_data_raw USING (tag_id) WHERE gps_data_raw.tag_id = deployments.tag_id AND ( ( (strftime(gps_data_raw.timestamp) &gt;= strftime(deployments.start_deployment)) AND (strftime(gps_data_raw.timestamp) &lt;= strftime(deployments.end_deployment)) ) OR ( (gps_data_raw.timestamp &gt;= deployments.start_deployment) AND (deployments.end_deployment IS NULL) ) ); Note that, because we populated this table with data from other existing tables, we ended up using INSERT INTO anyway and there was no need to use our trick with a temporary table. It should now be apparent that keeping the deployments table correctly filled out with no gaps or errors is of vital importance for the integrity of the whole telemetry database. Having the database set up this way means we never have to manually assign locations to animals, which would for sure lead to errors, but it also means that any analysis downstream hinges on keeping the deployments table up-to-date with new captures, tag retrievals, and deaths. This is just one example of how databases can save us time and ensure data integrity with minimal routine effort, but only if the data is curated with care in the first place. "],
["rsqlite.html", "Chapter 7 Linking Databases and R with RSQLite", " Chapter 7 Linking Databases and R with RSQLite library(DBI) dragons_db &lt;- dbConnect(RSQLite::SQLite(), &quot;../../Course Material/Data/dragons/dragons.db&quot;) dbExecute(dragons_db, &#39;DROP TABLE dragons&#39;) dbExecute(dragons_db, &#39;DROP TABLE morphometrics&#39;) dbExecute(dragons_db, &#39;DROP TABLE diet&#39;) dbExecute(dragons_db, &#39;DROP TABLE gps_data_raw&#39;) dbExecute(dragons_db, &#39;DROP TABLE gps_data&#39;) dbExecute(dragons_db, &#39;DROP TABLE captures&#39;) dbExecute(dragons_db, &#39;DROP TABLE deployments&#39;) dbExecute(dragons_db, &#39;DROP TABLE capture_sites&#39;) dbExecute(dragons_db, &#39;DROP TABLE tags&#39;) dbExecute(dragons_db, &quot;CREATE TABLE dragons ( dragon_id varchar(5) NOT NULL, sex char(1) CHECK (sex IN (&#39;M&#39;, &#39;F&#39;)), age_class varchar(8) CHECK (age_class IN (&#39;Juvenile&#39;, &#39;Subadult&#39;, &#39;Adult&#39;)), species varchar(50), PRIMARY KEY (dragon_id) );&quot;) dragons &lt;- read.csv(&quot;../../Course Material/Data/dragons/dragons.csv&quot;, stringsAsFactors = FALSE) names(dragons)[1] &lt;- &quot;dragon_id&quot; dbWriteTable(dragons_db, &quot;dragons&quot;, dragons, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM dragons LIMIT 10;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE tags ( tag_id char(6) NOT NULL PRIMARY KEY, brand varchar(50), status varchar(20) );&quot;) tags &lt;- read.csv(&quot;../../Course Material/Data/dragons/tags.csv&quot;) dbWriteTable(dragons_db, &quot;tags&quot;, tags, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM tags LIMIT 10;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE capture_sites ( site char(3) NOT NULL PRIMARY KEY, utm_x double, utm_y double );&quot;) capture_sites &lt;- read.csv(&quot;../../Course Material/Data/dragons/capture_sites.csv&quot;) names(capture_sites)[2:3] &lt;- c(&quot;utm_x&quot;, &quot;utm_y&quot;) dbWriteTable(dragons_db, &quot;capture_sites&quot;, capture_sites, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM capture_sites;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE captures ( capture_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), date text, site char(3), FOREIGN KEY(dragon_id) REFERENCES dragons(dragon_id) FOREIGN KEY(site) REFERENCES capture_sites(site) );&quot;) captures &lt;- read.csv(&quot;../../Course Material/Data/dragons/captures.csv&quot;) captures$capture_id &lt;- 1:nrow(captures) captures &lt;- captures[, c(&quot;capture_id&quot;, &quot;dragon&quot;, &quot;capture_date&quot;, &quot;capture_site&quot;)] names(captures)[2:4] &lt;- c(&quot;dragon_id&quot;, &quot;date&quot;, &quot;site&quot;) dbWriteTable(dragons_db, &quot;captures&quot;, captures, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM captures LIMIT 10;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE morphometrics ( measurement_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), date text, total_body_length_cm float, wingspan_cm float, tail_length_cm float, tarsus_length_cm float, claw_length_cm float, FOREIGN KEY (dragon_id) REFERENCES dragons(dragon_id) );&quot;) morphometrics &lt;- read.csv(&quot;../../Course Material/Data/dragons/morphometrics.csv&quot;) morphometrics$measurement_id &lt;- 1:nrow(morphometrics) morphometrics &lt;- morphometrics[, c(&quot;measurement_id&quot;, &quot;dragon&quot;, &quot;date&quot;, &quot;total_body_length_cm&quot;, &quot;wingspan_cm&quot;, &quot;tail_length_cm&quot;, &quot;tarsus_length_cm&quot;, &quot;claw_length_cm&quot;)] names(morphometrics)[2] &lt;- &quot;dragon_id&quot; dbWriteTable(dragons_db, &quot;morphometrics&quot;, morphometrics, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM morphometrics LIMIT 10;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE diet ( diet_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), sample_id varchar(8), date text, item_id integer, item varchar(50), FOREIGN KEY (dragon_id) REFERENCES dragons(dragon_id) );&quot;) diet &lt;- read.csv(&quot;../../Course Material/Data/dragons/diet.csv&quot;) diet$diet_id &lt;- 1:nrow(diet) diet &lt;- diet[, c(&quot;diet_id&quot;, &quot;dragon&quot;, &quot;sample_id&quot;, &quot;sample_dates&quot;, &quot;item_id&quot;, &quot;item&quot;)] names(diet)[c(2, 4)] &lt;- c(&quot;dragon_id&quot;, &quot;date&quot;) dbWriteTable(dragons_db, &quot;diet&quot;, diet, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM diet LIMIT 10;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE deployments ( deployment_id INTEGER PRIMARY KEY AUTOINCREMENT, dragon_id varchar(5), tag_id char(6), start_deployment text, end_deployment text, FOREIGN KEY(dragon_id) REFERENCES dragons(dragon_id) FOREIGN KEY(tag_id) REFERENCES tags(tag_id) );&quot;) deployments &lt;- read.csv(&quot;../../Course Material/Data/dragons/deployments.csv&quot;) deployments$deployment_id &lt;- 1:nrow(deployments) deployments &lt;- deployments[, c(&quot;deployment_id&quot;, &quot;dragon&quot;, &quot;tag&quot;, &quot;start_deploy&quot;, &quot;end_deploy&quot;)] names(deployments)[2:5] &lt;- c(&quot;dragon_id&quot;, &quot;tag_id&quot;, &quot;start_deployment&quot;, &quot;end_deployment&quot;) dbWriteTable(dragons_db, &quot;deployments&quot;, deployments, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM deployments LIMIT 10;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE gps_data_raw ( gps_id INTEGER PRIMARY KEY AUTOINCREMENT, tag_id char(6), timestamp text, utm_x double, utm_y double, FOREIGN KEY(tag_id) REFERENCES tags(tag_id) );&quot;) gps_data_raw &lt;- read.csv(&quot;../../Course Material/Data/dragons/telemetry_raw.csv&quot;) gps_data_raw$gps_id &lt;- 1:nrow(gps_data_raw) gps_data_raw &lt;- gps_data_raw[, c(&quot;gps_id&quot;, &quot;tag&quot;, &quot;timestamp&quot;, &quot;x&quot;, &quot;y&quot;)] names(gps_data_raw)[c(2, 4, 5)] &lt;- c(&quot;tag_id&quot;, &quot;utm_x&quot;, &quot;utm_y&quot;) dbWriteTable(dragons_db, &quot;gps_data_raw&quot;, gps_data_raw, append = TRUE) dbGetQuery(dragons_db, &quot;SELECT * FROM gps_data_raw LIMIT 10;&quot;) dbExecute(dragons_db, &quot;CREATE TABLE gps_data ( loc_id INTEGER PRIMARY KEY, tag_id char(6), dragon_id varchar(5), timestamp text, utm_x double, utm_y double, FOREIGN KEY (tag_id) REFERENCES tags(tag_id) FOREIGN KEY (dragon_id) REFERENCES dragons(dragon_id) );&quot;) dbExecute(dragons_db, &quot;INSERT INTO gps_data ( tag_id, dragon_id, timestamp, utm_x, utm_y) SELECT deployments.tag_id, deployments.dragon_id, gps_data_raw.timestamp, gps_data_raw.utm_x, gps_data_raw.utm_y FROM deployments LEFT JOIN gps_data_raw USING (tag_id) WHERE gps_data_raw.tag_id = deployments.tag_id AND ( ( (strftime(gps_data_raw.timestamp) &gt;= strftime(deployments.start_deployment)) AND (strftime(gps_data_raw.timestamp) &lt;= strftime(deployments.end_deployment)) ) OR ( (gps_data_raw.timestamp &gt;= deployments.start_deployment) AND (deployments.end_deployment IS NULL) ) );&quot;) dbGetQuery(dragons_db, &quot;SELECT * FROM gps_data LIMIT 10;&quot;) "],
["rmarkdown.html", "Chapter 8 Dynamic Documents with RMarkdown", " Chapter 8 Dynamic Documents with RMarkdown "],
["github-pages.html", "Chapter 9 Automatically Generated Websites with GitHub Pages 9.1 Bookdown 9.2 Publishing a book with GitHub Pages 9.3 Maintaining the website 9.4 References", " Chapter 9 Automatically Generated Websites with GitHub Pages We have seen how to knit RMarkdown documents into HTML, which is the standard format for web pages. From this to publishing your HTML documents as an actual website is a short step, thanks to GitHub Pages. GitHub Pages lets you turn your GitHub repositories into websites for free. Because it is GitHub, you can manage all the content of your website from RStudio and have it under version control, which means your website is reproducible. Sign us up, amirite?! Here, we are going to see how to create a web book using the R package bookdown and GitHub Pages. 9.1 Bookdown The R package bookdown is built on top of RMarkdown, so it uses the same syntax we have learned so far for writing documents. On top of it, bookdown lets you have multi-page output, cross-referencing between different pages, and it uses the GitBook style to create beautiful, professional-looking digital books. In fact, the book you are reading right now is built with bookdown! 9.1.1 Creating a book Once you have bookdown installed, you can create your first book by opening RStudio and creating a new Project. Select “New Directory”, and then “Book Project using bookdown”. Create a Project in a new directory. Choose “Book Project using bookdown” When you create the Project, bookdown automatically generates all the files you need for your book to function. These include: A .Rproj file, which is your RStudio Project; A README file; An index.Rmd file, which is the first section of your book and by default will be the home page if you publish it as a website; A series of numbered chapters as .Rmd files; Two .yml files, _bookdown.yml and _output.yml, which contain metadata (stuff like the file name of the book, the word used to refer to chapters within the book, etc;) A style.css file, which defines the style and appearance of the book; A preamble.tex file, with more customization options; A book.bib file, which contains the bibliography. Files automatically created by bookdown. In practice, you can ignore most of these files unless you want to customize the appearance and functioning of your book. If all you want to do is edit the content, all you need to care about are the .Rmd files. The other files are necessary for the book to work properly, but you can simply leave them where they are and not worry about them. To knit the book, go to the “Build” tab in RStudio, open the “Build Book” dropdown menu, and choose “bookdown::gitbook” as the output format. This format is compatible with GitHub Pages and will allow us to publish our book as a website. When we do this, bookdown will knit each .Rmd file into HTML format and save them into a new folder called \"_book\". Build bookdown. 9.2 Publishing a book with GitHub Pages To publish the book online using GitHub Pages, we are going to initialize a Git repository in our Project folder, link it up with a GitHub repository, and enable Pages. To enable Pages, we need to tell GitHub where to go find the .html files that compose our book. We have two options: we can put these files into a dedicated branch called “gh-pages”, or we can have them in a folder called “docs” on the main branch. The second option allows for a slightly simpler workflow, so we’ll go with that. 9.2.1 Step 1: Set up compatibility with GitHub Pages By default, bookdown puts the .html files it generates when knitting the book into the \"_book\" folder. Instead, we want these to be in a “docs” folder. We can go ahead and delete the _book folder with all its content (everything in this folder is generated when knitting the files so we can delete it without fear.) Then, we tell bookdown that it should save all the .html files into a folder called “docs” instead of the default \"_book\". We do this by opening the _bookdown.yml file and adding this line to the bottom of it: output_dir: “docs”. We also need to create a file called .nojekyll in the “docs” folder. You can do this by typing touch .nojekyll in your command line (make sure you’re in the Project folder), or you can simply use the notepad to create a new empty file, name it “.nojekyll”, and save it in “docs”. Make sure there’s no file extension (e.g., .txt) at the end of the filename. Notepad will complain and warn you that terrible things will happen if you remove the file extension, but actually they won’t. The reason why we need to create this “.nojekyll” file is because GitHub Pages will assume that your website is built using Jekyll, a static website builder, unless you tell it otherwise. Our website does not use Jekyll, so we tell GitHub that. 9.2.2 Step 2: Set up Git repository It’s time to put our book under version control. To do so, we open a terminal into our Project folder and we initialize a repository: git init After we create our .gitignore file, we can add and commit our files: git add --all git commit -m &quot;First commit&quot; Let’s also rename our main branch: git branch -M main 9.2.3 Step 3: Link Git repository to remote GitHub repository Now, we create an empty GitHub repository to link with our local one. Because we are setting this as the upstream for an existing repository, it must be completely empty, so make sure you are not creating a README file or anything (uncheck all those boxes). Also, make sure you choose the option for a Public repository. GitHub Pages are not available for private repositories. Back in the terminal, we link this newly created remote repository to our local copy (make sure you replace the URL below with the correct one): git remote add origin https://github.com/username/my-repo And we push our files to it: git push -u origin main 9.2.4 Step 4: Enable GitHub Pages The final step is to enable GitHub Pages on our repository. On GitHub, go to the repository Settings. Scroll down to the “GitHub Pages” section. Change the source to the docs folder in the main branch and save: Set up GitHub Pages to work from main, /docs. The site will be published within a few minutes at the address in the green box that just appeared on your screen. 9.3 Maintaining the website Once the website is live, it will be automatically updated any time we push updates to the .html files in the docs folder. Remember that modifying the .Rmd files is not enough for the website to update: we also need to knit the book so that bookdown will update the .html files. This is also a great reminder that you can play around with edits without them showing up on the website if you’re not ready to broadcast them: until you knit the book and push to the remote, the website won’t be updated. 9.4 References https://bookdown.org/yihui/bookdown/ "],
["intro-to-r.html", "Chapter 10 Introduction to R", " Chapter 10 Introduction to R "],
["troubleshooting.html", "Chapter 11 Troubleshooting in R", " Chapter 11 Troubleshooting in R "],
["renv.html", "Chapter 12 Working Environments in R", " Chapter 12 Working Environments in R "],
["tidyverse.html", "Chapter 13 Data Wrangling with tidyverse", " Chapter 13 Data Wrangling with tidyverse "],
["ggplot2.html", "Chapter 14 Data Visualization with ggplot2", " Chapter 14 Data Visualization with ggplot2 "],
["geospatial.html", "Chapter 15 Introduction to Geospatial Data in R", " Chapter 15 Introduction to Geospatial Data in R "],
["problem-decomposition.html", "Chapter 16 Problem Decomposition", " Chapter 16 Problem Decomposition "]
]
